name: Update Isometric Data

on:
  # Run daily at 5 AM UTC (before any downstream API updates)
  schedule:
    - cron: '0 5 * * *'
  
  # Trigger on push to main (for manual updates)
  push:
    branches:
      - main
    paths:
      - 'scripts/**'
      - 'removal_db_data/**'
      - 'requirements.txt'
      - '.github/workflows/update-data.yml'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:

env:
  AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
  AWS_REGION: us-east-1

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write  # Required to push changes
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli
      
      - name: Fetch latest data from Isometric API
        run: python scripts/fetch_isometric.py
      
      - name: Process data into Parquet files
        run: python scripts/run_pipeline.py
      
      - name: Check for changes
        id: check_changes
        run: |
          git diff --quiet output/ && echo "changed=false" >> $GITHUB_OUTPUT || echo "changed=true" >> $GITHUB_OUTPUT
      
      # Upload to AWS S3 (if secrets are configured)
      - name: Upload to AWS S3
        if: env.AWS_S3_BUCKET != ''
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "Uploading Parquet files to S3..."
          aws s3 cp output/credits-isometric.parquet s3://$AWS_S3_BUCKET/isometric/credits.parquet
          aws s3 cp output/projects-isometric.parquet s3://$AWS_S3_BUCKET/isometric/projects.parquet
          
          # Also upload timestamped versions for history
          TIMESTAMP=$(date -u +'%Y-%m-%d')
          aws s3 cp output/credits-isometric.parquet s3://$AWS_S3_BUCKET/isometric/history/credits-$TIMESTAMP.parquet
          aws s3 cp output/projects-isometric.parquet s3://$AWS_S3_BUCKET/isometric/history/projects-$TIMESTAMP.parquet
          
          echo "âœ… Uploaded to s3://$AWS_S3_BUCKET/isometric/"
      
      - name: Commit and push if changed
        if: steps.check_changes.outputs.changed == 'true'
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add output/*.parquet
          git add raw/*.csv
          git add raw/fetch_metadata.json
          git commit -m "ðŸ”„ Daily data update - $(date -u +'%Y-%m-%d %H:%M UTC')"
          git push
      
      - name: Summary
        run: |
          echo "## Data Update Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Files Updated" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          ls -la output/*.parquet >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Record Counts" >> $GITHUB_STEP_SUMMARY
          python -c "
          import pandas as pd
          credits = pd.read_parquet('output/credits-isometric.parquet')
          projects = pd.read_parquet('output/projects-isometric.parquet')
          print(f'- Credits: {len(credits):,} transactions')
          print(f'- Projects: {len(projects):,} projects')
          print(f'- Total issued: {credits[credits[\"transaction_type\"]==\"issuance\"][\"quantity\"].sum():,.0f} tonnes')
          " >> $GITHUB_STEP_SUMMARY
          if [ -n "$AWS_S3_BUCKET" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### S3 Upload" >> $GITHUB_STEP_SUMMARY
            echo "âœ… Uploaded to \`s3://$AWS_S3_BUCKET/isometric/\`" >> $GITHUB_STEP_SUMMARY
          fi

